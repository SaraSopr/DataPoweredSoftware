{
  "cells": [
    {
      "metadata": {
        "id": "2N2rB1mS10P9"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 Data Exploration\n",
        "    a. Explore the dataset by displaying the first few rows, summary statistics, and data types of each column.\n",
        "    b. Identify missing values, outliers, and unique values in categorical columns."
      ]
    },
    {
      "metadata": {
        "id": "HphbxRHd10P_"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 store sales\n",
        "### 1.1.a Explore the dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pFuD__CE10P_"
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as mtick\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "from qtconsole.mainwindow import background\n",
        "from scipy import stats\n",
        "\n",
        "DATA_DIR = Path(\"data/assigment_1/store-sales-item-time-series\")\n",
        "\n",
        "stores = pd.read_csv(DATA_DIR / \"stores.csv\",\n",
        "                          names=[\"store_nbr\", \"city\", \"state\", \"type\", \"cluster\" ],\n",
        "                          header=0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "FPwpK39V10QA"
      },
      "cell_type": "markdown",
      "source": [
        "#### Oil"
      ]
    },
    {
      "metadata": {
        "id": "Grg-BvQo10QA"
      },
      "cell_type": "code",
      "source": [
        "# 1. Overview\n",
        "\n",
        "\n",
        "oil_df = pd.read_csv(DATA_DIR / \"oil.csv\", names=[\"date\", \"oil_price\"], header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(oil_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(oil_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(oil_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jFCDE2wL10QB"
      },
      "cell_type": "code",
      "source": [
        "print(\"=== Time Period: ===\")\n",
        "print(\"Begin:\", oil_df[\"date\"].min(), \"\\nEnd:\", oil_df[\"date\"].max())\n",
        "print(\"Tage insgesamt:\", oil_df[\"date\"].nunique())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "WzPY48k910QB"
      },
      "cell_type": "markdown",
      "source": [
        "#### holiday"
      ]
    },
    {
      "metadata": {
        "id": "9Iw2qGR410QB"
      },
      "cell_type": "code",
      "source": [
        "holidays_df = pd.read_csv(DATA_DIR / \"holidays_events.csv\",\n",
        "                          names=[\"date\", \"type\", \"local\", \"local-name\", \"description\", \"transferred\", ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(holidays_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(holidays_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(holidays_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "luHeP8uf10QB"
      },
      "cell_type": "code",
      "source": [
        "# 2. Missing Values\n",
        "print(\"=== How much NAs per Col: ===\")\n",
        "print(holidays_df.isnull().sum())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Cj2fqmth10QB"
      },
      "cell_type": "markdown",
      "source": [
        "#### Sample submission\n"
      ]
    },
    {
      "metadata": {
        "id": "5Ciat5ah10QC"
      },
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv(DATA_DIR / \"sample_submission.csv\",\n",
        "                          names=[\"id\", \"sales\" ],\n",
        "                          header=0)\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(sample_submission.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(sample_submission.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(sample_submission.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4yBtB-Sr10QC"
      },
      "cell_type": "markdown",
      "source": [
        "#### Stores"
      ]
    },
    {
      "metadata": {
        "id": "8dx6Fw3U10QC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(stores.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(stores.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(stores.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "X8o6xYa_10QC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "store_counts = stores['type'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "store_counts.plot(kind='bar')\n",
        "plt.title(\"Distribution of Stores by Type\")\n",
        "plt.xlabel(\"Store Type\")\n",
        "plt.ylabel(\"Number of Stores\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fzS6BwI110QC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cluster_type_ct = pd.crosstab(stores['cluster'], stores['type'])\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cluster_type_ct, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
        "plt.title(\"Cluster vs. Store Type\")\n",
        "plt.ylabel(\"Cluster\")\n",
        "plt.xlabel(\"Store Type\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ElfRg2y510QC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "city_cluster_ct = pd.crosstab(stores['city'], stores['cluster'])\n",
        "\n",
        "values = range(int(city_cluster_ct.values.max()) + 1)\n",
        "cmap = ListedColormap(sns.color_palette(\"YlGnBu\", len(values)))\n",
        "norm = BoundaryNorm(values, cmap.N)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(\n",
        "    city_cluster_ct,\n",
        "    cmap=cmap,\n",
        "    norm=norm,\n",
        "    cbar=True,\n",
        "    linewidths=0.5,\n",
        "    linecolor=\"gray\",\n",
        "    annot=True, fmt=\"d\"\n",
        ")\n",
        "plt.title(\"City vs. Cluster Distribution (Discrete)\")\n",
        "plt.ylabel(\"City\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Qw3L7pDq10QC"
      },
      "cell_type": "markdown",
      "source": [
        "#### transaction\n"
      ]
    },
    {
      "metadata": {
        "id": "6w5gwjQ-10QD"
      },
      "cell_type": "code",
      "source": [
        "transaction_df = pd.read_csv(DATA_DIR / \"transactions.csv\",\n",
        "                          names=[\"date\", \"store_nbr\", \"transaction\" ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(transaction_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(transaction_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(transaction_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "69KxZaV910QD"
      },
      "cell_type": "code",
      "source": [
        "# count unique store_nbr\n",
        "transaction_df['store_nbr'].nunique()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bLVzryBM10QD"
      },
      "cell_type": "code",
      "source": [
        "transactions_per_day = transaction_df.groupby('date')['transaction'].sum()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "transactions_per_day.plot()\n",
        "plt.title(\"Total Transactions per Day\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Transactions\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "A2ElgQX910QD"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "transactions_per_day.plot(alpha=0.4, label=\"Daily\")\n",
        "transactions_per_day.rolling(7).mean().plot(label=\"7-day Avg\")\n",
        "transactions_per_day.rolling(14).mean().plot(label=\"14-day Avg\")\n",
        "transactions_per_day.rolling(30).mean().plot(label=\"30-day Avg\")\n",
        "plt.title(\"Total Transactions per Day with Rolling Averages\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Transactions\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cxiDoiay10QD"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train/ Test"
      ]
    },
    {
      "metadata": {
        "id": "ExoRNxQt10QD"
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(DATA_DIR / \"train.csv\",\n",
        "                          names=[\"id\", \"date\", \"store_nbr\", \"family\",\"sales\", \"onpromotion\" ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(train_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(train_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VcNprnnX10QD"
      },
      "cell_type": "code",
      "source": [
        "train_df['family'].nunique()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Kbtx2pDg10QD"
      },
      "cell_type": "code",
      "source": [
        "# missing values\n",
        "train_df.isna().sum()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "IdVC0zJ110QD"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "daily_sales = train_df.groupby('date')['sales'].sum()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "ax = daily_sales.plot()\n",
        "\n",
        "# values into millions\n",
        "ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'{float(x/1e6)}M'))\n",
        "\n",
        "plt.title(\"Total Sales per Day\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "rJN-7jqP10QD"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "weekday_sales = train_df.groupby(train_df['date'].dt.day_name())['sales'].sum()\n",
        "\n",
        "# define order fpr weekdays\n",
        "order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "weekday_sales = weekday_sales.reindex(order)\n",
        "\n",
        "# values into millions\n",
        "ax = weekday_sales.plot.bar()\n",
        "ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'{float(x/1e6)}M'))\n",
        "ax.xaxis.set_tick_params(rotation=45)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "AjlyjCvD10QE"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.b Identifiy missing data and outliers"
      ]
    },
    {
      "metadata": {
        "id": "8MXsgi4T10QE"
      },
      "cell_type": "markdown",
      "source": [
        "#### missing data"
      ]
    },
    {
      "metadata": {
        "id": "76S1fif_10QE"
      },
      "cell_type": "code",
      "source": [
        "# Missing oil\n",
        "print(\"=== How much NAs per Col: ===\")\n",
        "print(oil_df.isnull().sum())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Fv0Wv2G510QE"
      },
      "cell_type": "code",
      "source": [
        "# Visualization to missing oil data:\n",
        "import matplotlib.pyplot as plt\n",
        "s = oil_df.set_index(\"date\")[\"oil_price\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,5))\n",
        "s.plot(ax=ax)\n",
        "\n",
        "for x in s.index[s.isna()]:\n",
        "    ax.axvline(x, linestyle=\"--\", linewidth=0.8, alpha=0.3, color=\"tab:orange\")\n",
        "\n",
        "ax.set_title(\"Daily Oil Price\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cuy4wTTu10QE"
      },
      "cell_type": "code",
      "source": [
        "tpd = (transaction_df\n",
        "       .groupby(['store_nbr', 'date'], as_index=False)['transaction']\n",
        "       .sum()\n",
        "       .sort_values(['store_nbr', 'date']))\n",
        "tpd['date'] = pd.to_datetime(tpd['date']).dt.normalize()\n",
        "\n",
        "\n",
        "full_idx = pd.date_range(tpd['date'].min(), tpd['date'].max(), freq='D')\n",
        "\n",
        "\n",
        "dates_by_store = tpd.groupby('store_nbr')['date'].unique()\n",
        "missing_counts = {s: len(full_idx.difference(pd.DatetimeIndex(dates)))\n",
        "                  for s, dates in dates_by_store.items()}\n",
        "stores_with_gaps = [s for s, cnt in missing_counts.items() if cnt > 0]\n",
        "\n",
        "stores_to_plot = sorted(stores_with_gaps, key=lambda s: missing_counts[s], reverse=True)\n",
        "\n",
        "# Subplots\n",
        "n = len(stores_to_plot)\n",
        "cols = 3 if n > 1 else 1\n",
        "rows = math.ceil(n / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14, 3.2*rows), sharex=True)\n",
        "axes = axes.flatten() if n > 1 else [axes]\n",
        "\n",
        "for ax, s in zip(axes, stores_to_plot):\n",
        "    sdf = (tpd[tpd['store_nbr'] == s]\n",
        "           .set_index('date')\n",
        "           .reindex(full_idx))\n",
        "    ax.plot(sdf.index, sdf['transaction'], linewidth=0.5)\n",
        "\n",
        "    miss = sdf['transaction'].isna()\n",
        "    if miss.any():\n",
        "        ymin, ymax = ax.get_ylim()\n",
        "        ax.scatter(sdf.index[miss], [ymin]*miss.sum(), marker='v', s=16, alpha=0.7, color=\"red\")\n",
        "    ax.set_title(f\"Store {s}  (missing days: {missing_counts[s]})\", fontsize=10)\n",
        "\n",
        "for ax in axes[n:]:\n",
        "    ax.axis('off')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "JhChGbaj10QE"
      },
      "cell_type": "markdown",
      "source": [
        "Most stores have 10-12 days missing around Christmas/ New Year, but some stores have a lot more missing data."
      ]
    },
    {
      "metadata": {
        "id": "5xguN_1210QE"
      },
      "cell_type": "code",
      "source": [
        "tpd_df = transaction_df.copy()\n",
        "tpd_df['date'] = pd.to_datetime(tpd_df['date']).dt.normalize()\n",
        "\n",
        "tpd = (tpd_df\n",
        "       .groupby(['store_nbr', 'date'], as_index=False)['transaction']\n",
        "       .sum()\n",
        "       .sort_values(['store_nbr', 'date']))\n",
        "\n",
        "\n",
        "stores_to_plot = sorted(tpd['store_nbr'].unique().tolist())\n",
        "\n",
        "\n",
        "n = len(stores_to_plot)\n",
        "cols = 4 if n >= 12 else 3 if n >= 6 else 2 if n > 1 else 1\n",
        "rows = math.ceil(n / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(3.2*cols, 3.8*rows))\n",
        "axes = axes.flatten() if n > 1 else [axes]\n",
        "\n",
        "for ax, store in zip(axes, stores_to_plot):\n",
        "    svals = tpd.loc[tpd['store_nbr'] == store, 'transaction']\n",
        "    sns.boxplot(y=svals, ax=ax, fliersize=2)\n",
        "    ax.set_title(f\"Store {store} (n={svals.notna().sum()})\", fontsize=9)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "\n",
        "for ax in axes[len(stores_to_plot):]:\n",
        "    ax.axis('off')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jdUb6PE810QE"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like there are outliers in every store transaction."
      ]
    },
    {
      "metadata": {
        "id": "jfi2XReU10QE"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Data Cleaning\n",
        "    a. Handling Missing Values\n",
        "    b. Choose appropriate methods to handle missing values (e.g., mean/median imputation for numerical data, mode imputation for categorical data, or deletion of rows/columns).\n",
        "    c. Justify your choices for handling missing data."
      ]
    },
    {
      "metadata": {
        "id": "Ctn6cqGM10QF"
      },
      "cell_type": "code",
      "source": [
        "oil_df['date'] = pd.to_datetime(oil_df['date'])\n",
        "oil_df = oil_df.sort_values('date')\n",
        "\n",
        "\n",
        "oil_df['oil_price'] = oil_df['oil_price'].interpolate(\n",
        "    method=\"spline\", order=5\n",
        ")\n",
        "\n",
        "s = oil_df.set_index(\"date\")[\"oil_price\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,5))\n",
        "s.plot(ax=ax)\n",
        "\n",
        "\n",
        "ax.set_title(\"Daily Oil Price (Spline interpolation)\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "udtZrxHB9Gey"
      },
      "cell_type": "markdown",
      "source": [
        "In our dataset, only a few short gaps in the oil price data occurred. Since oil prices typically evolve continuously over time without abrupt jumps, interpolation is a suitable approach in this context. We chose spline interpolation because it produces smoother transitions than linear interpolation and thus reflects the economic trajectory more realistically. This allows us to fill the missing values consistently without significantly distorting the statistical properties of the series."
      ]
    },
    {
      "metadata": {
        "id": "khH9Nk5k9Gez"
      },
      "cell_type": "code",
      "source": [
        "# handle missing data in store\n",
        "\n",
        "# implement holiday feature to explain missing data\n",
        "\"\"\"\n",
        "    is_holiday 1: holiday.type in {holiday, additonal, bridge, Event} && holiday.transfered == false && holiday.locale == National\n",
        "    is_holiday 1: holiday.type in {holiday, additonal, bridge, Event} && holiday.transfered == false && (holiday.locale == Regional && stores[transactions.str_nbr].state ==  holiday.locale_name)\n",
        "    is_holiday 1: holiday.type in {holiday, additonal, bridge, Event} && holiday.transfered == false && (holiday.locale == Local && stores[transactions.str_nbr].city ==  holiday.locale_name)\n",
        "\n",
        "    is_holiday 0: holiday.type == workday || holiday.transfered == true\n",
        "    is_holiday 0: default with no holiday in the file\n",
        "\n",
        "    is_holiday 1: holiday.type == Transfered\n",
        "\n",
        "    => holiday.type == Transfered :: holiday.type to holiday\n",
        "    => holiday.transfered == true :: holiday.type to workday\n",
        "\"\"\"\n",
        "\n",
        "transactions = pd.read_csv(DATA_DIR / \"transactions.csv\",\n",
        "                          names=[\"date\", \"store_nbr\", \"transaction\" ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "holidays = pd.read_csv(DATA_DIR / \"holidays_events.csv\",\n",
        "                          names=[\"date\", \"type\", \"locale\", \"locale_name\", \"description\", \"transferred\", ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "stores = pd.read_csv(DATA_DIR / \"stores.csv\",\n",
        "                          names=[\"store_nbr\", \"city\", \"state\", \"type\", \"cluster\" ],\n",
        "                          header=0)\n",
        "\n",
        "\n",
        "transactions['date'] = pd.to_datetime(transactions['date']).dt.normalize()\n",
        "holidays['date'] = pd.to_datetime(holidays['date']).dt.normalize()\n",
        "\n",
        "# delete an value:\n",
        "transactions = transactions[transactions['date'] != pd.to_datetime(\"2013-01-01\")]\n",
        "transactions_grouped = (transactions\n",
        "                            .groupby(['store_nbr', 'date'], as_index=False)['transaction']\n",
        "                            .sum()\n",
        "                            .sort_values(['store_nbr', 'date']))\n",
        "\n",
        "# holidays preprocessing\n",
        "holidays_clean = holidays.copy()\n",
        "\n",
        "# transfers: make them Holiday\n",
        "holidays_clean.loc[holidays_clean['type'] == \"Transfer\", 'type'] = \"Holiday\"\n",
        "\n",
        "# transferred==True → make them Work Day\n",
        "holidays_clean.loc[holidays_clean['transferred'] == True, 'type'] = \"Work Day\"\n",
        "\n",
        "# join holidays onto transactions\n",
        "df = transactions_grouped.merge(stores, on=\"store_nbr\", how=\"left\")\n",
        "\n",
        "# add holiday info\n",
        "df = df.merge(holidays_clean[['date','type','locale','locale_name','transferred']],\n",
        "              on=\"date\", how=\"left\",\n",
        "\n",
        "              suffixes=(\"\", \"_holiday\"))\n",
        "\n",
        "\n",
        "# default: no holiday\n",
        "df['is_holiday'] = 0\n",
        "\n",
        "# rule 1: national holiday\n",
        "mask_nat = (df['type_holiday'].isin([\"Holiday\",\"Additional\",\"Bridge\"])) & (df['locale']==\"National\")\n",
        "df.loc[mask_nat, 'is_holiday'] = 1\n",
        "\n",
        "# rule 2: regional holiday\n",
        "mask_reg = (df['type_holiday'].isin([\"Holiday\",\"Additional\",\"Bridge\"])) & (df['locale']==\"Regional\") & (df['state']==df['locale_name'])\n",
        "df.loc[mask_reg, 'is_holiday'] = 1\n",
        "\n",
        "# rule 3: local holiday\n",
        "mask_loc = (df['type_holiday'].isin([\"Holiday\",\"Additional\",\"Bridge\"])) & (df['locale']==\"Local\") & (df['city']==df['locale_name'])\n",
        "df.loc[mask_loc, 'is_holiday'] = 1\n",
        "\n",
        "# remove city, state, type, cluster, type_holiday, locale, locale_name, transferred\n",
        "\n",
        "df = df[['date','store_nbr','transaction','is_holiday']]\n",
        "print(df.tail(20))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2vDwXuGn9Ge5"
      },
      "cell_type": "code",
      "source": [
        "# Build store open feature\n",
        "\"\"\"\n",
        "    store_open 0: store is closed if data is missing for more than 10 continuing days\n",
        "        => add a transaction on dates with 0\n",
        "    store_open 1: store is open if data is available for at least 10 continuing days\n",
        "\"\"\"\n",
        "global_start = df['date'].min()\n",
        "global_end   = df['date'].max()\n",
        "\n",
        "full_cal = []\n",
        "for s in df['store_nbr'].unique():\n",
        "    g = (df.loc[df['store_nbr'] == s, ['date','transaction','is_holiday']]\n",
        "           .copy()\n",
        "           .sort_values('date'))\n",
        "\n",
        "    g = (g.groupby('date', as_index=False)\n",
        "           .agg(transaction=('transaction','sum'),\n",
        "                is_holiday =('is_holiday','max')))\n",
        "\n",
        "    idx = pd.DataFrame({'date': pd.date_range(global_start, global_end, freq='D')})\n",
        "    g_full = idx.merge(g, on='date', how='left')\n",
        "\n",
        "    g_full['store_nbr'] = s\n",
        "\n",
        "    # Holiday-Flag for added days\n",
        "    g_full['is_holiday'] = g_full['is_holiday'].fillna(0).astype(int)\n",
        "\n",
        "    full_cal.append(g_full)\n",
        "\n",
        "df_full = pd.concat(full_cal, ignore_index=True)\n",
        "\n",
        "\n",
        "def run_mask(series, cond, min_len=10):\n",
        "    m = cond(series)\n",
        "    grp = (m != m.shift()).cumsum()\n",
        "    runlen = m.groupby(grp).transform('size')\n",
        "    return m & (runlen >= min_len)\n",
        "\n",
        "df_full = df_full.sort_values(['store_nbr','date'])\n",
        "\n",
        "open_run = (df_full.groupby('store_nbr')['transaction']\n",
        "                  .apply(lambda s: run_mask(s, pd.Series.notna, min_len=10))\n",
        "                  .reset_index(level=0, drop=True))\n",
        "\n",
        "closed_run = (df_full.groupby('store_nbr')['transaction']\n",
        "                    .apply(lambda s: run_mask(s, pd.Series.isna, min_len=10))\n",
        "                    .reset_index(level=0, drop=True))\n",
        "df_full['store_open'] = np.where(open_run, 1,\n",
        "                          np.where(closed_run, 0, np.nan))\n",
        "\n",
        "df_full['store_open'] = (\n",
        "    df_full\n",
        "      .groupby('store_nbr', group_keys=False)['store_open']\n",
        "      .apply(lambda s: s.ffill().bfill())\n",
        "      .fillna(0)\n",
        "      .astype(int)\n",
        ")\n",
        "\n",
        "df_full.loc[(df_full['store_open']==0) & (df_full['transaction'].isna()), 'transaction'] = 0\n",
        "\n",
        "def interp_per_store(g, max_gap=5):\n",
        "    g = g.sort_values('date').set_index('date')\n",
        "    allowed = (g['store_open'] == 1) & (g['is_holiday'] == 0)\n",
        "    y = g['transaction'].copy()\n",
        "    # only interpolate allowed data\n",
        "    y_allowed = y.where(allowed)\n",
        "    y_filled = y_allowed.interpolate(method='time',\n",
        "                                     limit=max_gap,\n",
        "                                     limit_area='inside')\n",
        "    g.loc[allowed & y.isna(), 'transaction'] = y_filled\n",
        "    return g.reset_index()\n",
        "\n",
        "df_full = df_full.groupby('store_nbr', group_keys=False).apply(interp_per_store, max_gap=5)\n",
        "\n",
        "print(df_full.tail(20))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Vg_RmgH79Ge6"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning – Transactions\n",
        "\n",
        "- A single value (2013-01-01) was removed, since data was only available for one store.\n",
        "- Holiday data was integrated and marked with `is_holiday` to explain transaction peaks.\n",
        "- A complete daily calendar was built for each store and `store_open` was derived:\n",
        "  - `0` if ≥10 consecutive days without data (closed, transactions = 0).\n",
        "  - `1` if ≥10 consecutive days with data (open).\n",
        "- Missing values in open, non-holiday phases were only interpolated for short gaps (≤5 days) using time-based interpolation.\n",
        "\n",
        "➡️ This way, real closures and events remain intact, while small gaps are handled consistently.\n"
      ]
    },
    {
      "metadata": {
        "id": "lq4vBt1B9Ge6"
      },
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "#                   VISUALIZATION\n",
        "# ===================================================\n",
        "stores_unique = df_full['store_nbr'].unique()\n",
        "\n",
        "for store in stores_unique:\n",
        "    store_df = df_full[df_full['store_nbr'] == store].sort_values('date')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16,5))\n",
        "    ax.plot(store_df['date'], store_df['transaction'], label=\"Transactions\", color=\"blue\", linewidth=0.4, alpha=1)\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"holiday_dates = store_df.loc[store_df['is_holiday'] == 1, 'date']\n",
        "    for hday in holiday_dates:\n",
        "        ax.axvline(hday, color=\"orange\", linestyle=\"--\", linewidth=0.5)\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    closed_dates = store_df.loc[store_df['store_open'] == 0, 'date']\n",
        "    for cdate in closed_dates:\n",
        "        ax.axvline(cdate, color=\"orange\", linestyle=\"-\", linewidth=0.5)\n",
        "    \"\"\"\n",
        "    open_but_no_data = store_df.loc[\n",
        "        (store_df['store_open'] == 1) &\n",
        "        (store_df['is_holiday'] == 0) &\n",
        "        (store_df['transaction'].isna()),\n",
        "        'date'\n",
        "    ]\n",
        "    for obnd in open_but_no_data:\n",
        "        ax.axvline(obnd, color=\"red\", linestyle=\"-\", linewidth=0.5)\n",
        "\n",
        "    ax.set_title(f\"Store {store} - Transactions with Holidays\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Transactions\")\n",
        "    ax.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "vnhpBMRj9Ge6"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HillBxOF10QF"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Handling Outliers\n",
        "    a. Detect outliers using methods such as the IQR method or Z-score.\n",
        "    b. Decide whether to remove, cap, or transform the outliers. Justify your decisions."
      ]
    },
    {
      "metadata": {
        "id": "M6j7ZN5h10QF"
      },
      "cell_type": "code",
      "source": [
        "# 1) Load data\n",
        "train = pd.read_csv(\"train.csv\", parse_dates=[\"date\"])\n",
        "transactions = pd.read_csv(\"transactions.csv\", parse_dates=[\"date\"])\n",
        "oil = pd.read_csv(\"oil.csv\", parse_dates=[\"date\"])\n",
        "\n",
        "# 2) Merge data\n",
        "df = train.merge(transactions, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
        "df = df.merge(oil, on=\"date\", how=\"left\")\n",
        "\n",
        "# Numeric columns of interest\n",
        "num_cols = [\"sales\", \"onpromotion\", \"transactions\", \"dcoilwtico\"]\n",
        "\n",
        "# 3) Basic cleaning\n",
        "for col in [\"sales\", \"onpromotion\", \"transactions\"]:\n",
        "    # Negative values are not valid → set to NaN\n",
        "    df.loc[df[col] < 0, col] = np.nan\n",
        "\n",
        "# Interpolate missing oil prices\n",
        "oil_sorted = oil.sort_values(\"date\").set_index(\"date\")\n",
        "oil_sorted[\"dcoilwtico\"] = oil_sorted[\"dcoilwtico\"].interpolate(method=\"time\")\n",
        "df = df.drop(columns=[\"dcoilwtico\"]).merge(oil_sorted.reset_index(), on=\"date\", how=\"left\")\n",
        "\n",
        "# 4) Outlier detection (IQR and Z-Score)\n",
        "iqr_bounds, iqr_counts, z_counts = {}, {}, {}\n",
        "\n",
        "for col in num_cols:\n",
        "    series = df[col].dropna()\n",
        "    q1, q3 = series.quantile(0.25), series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
        "    iqr_bounds[col] = (lower, upper)\n",
        "    iqr_counts[col] = int(((df[col] < lower) | (df[col] > upper)).sum())\n",
        "\n",
        "    z = np.abs(stats.zscore(series, nan_policy=\"omit\"))\n",
        "    z_counts[col] = int((z > 3).sum())\n",
        "\n",
        "print(\"Outlier counts (IQR):\", iqr_counts)\n",
        "print(\"Outlier counts (Z-Score):\", z_counts)\n",
        "\n",
        "# 5) Visual inspection\n",
        "def plot_hist(col, bins=60):\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.hist(df[col].dropna(), bins=bins)\n",
        "    plt.title(f\"Histogram of {col}\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_box(col):\n",
        "    plt.figure(figsize=(7,2.5))\n",
        "    plt.boxplot(df[col].dropna(), vert=False, whis=1.5)\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()\n",
        "\n",
        "for c in num_cols:\n",
        "    plot_hist(c)\n",
        "    plot_box(c)\n",
        "\n",
        "# 6) Outlier handling strategy\n",
        "df_handled = df.copy()\n",
        "\n",
        "# sales: keep raw, add log1p transformation\n",
        "df_handled[\"sales_log1p\"] = np.log1p(df_handled[\"sales\"])\n",
        "\n",
        "# onpromotion: cap at 99.5th percentile\n",
        "p995_onpromo = df_handled[\"onpromotion\"].quantile(0.995)\n",
        "df_handled[\"onpromotion_capped\"] = np.where(\n",
        "    df_handled[\"onpromotion\"] > p995_onpromo, p995_onpromo, df_handled[\"onpromotion\"]\n",
        ")\n",
        "\n",
        "# transactions: cap at 99.5th percentile\n",
        "p995_trans = df_handled[\"transactions\"].quantile(0.995)\n",
        "df_handled[\"transactions_capped\"] = np.where(\n",
        "    df_handled[\"transactions\"] > p995_trans, p995_trans, df_handled[\"transactions\"]\n",
        ")\n",
        "\n",
        "# oil price: winsorize between 1st and 99th percentiles\n",
        "p01_oil, p99_oil = df_handled[\"dcoilwtico\"].quantile([0.01, 0.99])\n",
        "df_handled[\"dcoilwtico_winsor\"] = df_handled[\"dcoilwtico\"].clip(lower=p01_oil, upper=p99_oil)\n",
        "\n",
        "print(\"Caps applied -> onpromotion 99.5%:\", p995_onpromo,\n",
        "      \"| transactions 99.5%:\", p995_trans,\n",
        "      \"| oil [1%, 99%]:\", (p01_oil, p99_oil))\n",
        "\n",
        "# 7) Save cleaned dataset\n",
        "df_handled.to_csv(\"train_outlier_handled.csv\", index=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YVxIqgca10QU"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 Data Transformation\n",
        "    a. Encoding Categorical Data\n",
        "        i. Apply label encoding or one-hot encoding to transform categorical data into numerical form.\n",
        "        ii. Justify your choice of encoding method.\n",
        "    b. Feature Scaling\n",
        "        i. Apply feature scaling techniques such as normalization (Min-Max scaling) or standardization (Z-score normalization) to the dataset.\n",
        "        ii. Explain why feature scaling is necessary and how it impacts the model.\n"
      ]
    },
    {
      "metadata": {
        "id": "PV74sH1E10QU"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eYdIHSGQ10QU"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 Data Splitting\n",
        "    a. Split the preprocessed dataset into training and testing sets. Typically, an 80-20 or 70-30 split is used.\n"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkICzAf110QU",
        "outputId": "631c3c61-b11c-4d4d-b045-dc1a7aff4535"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Parameters\n",
        "train_fraction = 0.8\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"train.csv\", parse_dates=[\"date\"])\n",
        "test_df = pd.read_csv(\"test.csv\", parse_dates=[\"date\"])\n",
        "\n",
        "# Combine ALL datasets (test.csv will have NaN in sales column)\n",
        "all_cols = sorted(set(train_df.columns).union(set(test_df.columns)))\n",
        "train_df = train_df.reindex(columns=all_cols)\n",
        "test_df = test_df.reindex(columns=all_cols)\n",
        "full_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
        "\n",
        "# Sort by date chronologically\n",
        "full_df = full_df.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Chronological split on ALL data: first 80% by time for training\n",
        "n = len(full_df)  # This is 3,029,400\n",
        "split_idx = int(n * train_fraction)  # This is 2,423,520\n",
        "\n",
        "train_part = full_df.iloc[:split_idx]\n",
        "test_part = full_df.iloc[split_idx:]\n",
        "\n",
        "# Separate features and target (some sales values will be NaN)\n",
        "X_train = train_part.drop(\"sales\", axis=1)\n",
        "y_train = train_part[\"sales\"]\n",
        "X_test = test_part.drop(\"sales\", axis=1)\n",
        "y_test = test_part[\"sales\"]\n",
        "\n",
        "# Results\n",
        "print(\"Total data:\", n)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "print(\"NaN values in y_train:\", y_train.isna().sum())\n",
        "print(\"NaN values in y_test:\", y_test.isna().sum())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total data: 3029400\n",
            "X_train shape: (2423520, 5)\n",
            "X_test shape: (605880, 5)\n",
            "y_train shape: (2423520,)\n",
            "y_test shape: (605880,)\n",
            "NaN values in y_train: 0\n",
            "NaN values in y_test: 28512\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "    b. Explain the importance of splitting the data and how it prevents overfitting."
      ],
      "metadata": {
        "id": "jRFJhzIK2FuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset into training (80%) and testing (20%) sets. The training set is used to teach the model, while the testing set evaluates its performance on unseen data. This prevents overfitting, ensuring the model generalizes well rather than just memorizing the training data."
      ],
      "metadata": {
        "id": "5bnA_nBI2cT0"
      }
    },
    {
      "metadata": {
        "id": "oKm_O51k10QU"
      },
      "cell_type": "markdown",
      "source": [
        "# 6 Bonus\n",
        "Apply dimensionality reduction techniques such as Principal\n",
        "Component Analysis (PCA) and discuss how it affects the dataset."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}