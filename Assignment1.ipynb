{
  "cells": [
    {
      "metadata": {
        "id": "2N2rB1mS10P9"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 Data Exploration\n",
        "    a. Explore the dataset by displaying the first few rows, summary statistics, and data types of each column.\n",
        "    b. Identify missing values, outliers, and unique values in categorical columns."
      ]
    },
    {
      "metadata": {
        "id": "HphbxRHd10P_"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 store sales\n",
        "### 1.1.a Explore the dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pFuD__CE10P_"
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as mtick\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "FPwpK39V10QA"
      },
      "cell_type": "markdown",
      "source": [
        "#### Oil"
      ]
    },
    {
      "metadata": {
        "id": "Grg-BvQo10QA"
      },
      "cell_type": "code",
      "source": [
        "# 1. Overview\n",
        "\n",
        "DATA_DIR = Path(\"data/assigment_1/store-sales-item-time-series\")\n",
        "\n",
        "oil_df = pd.read_csv(DATA_DIR / \"oil.csv\", names=[\"date\", \"oil_price\"], header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(oil_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(oil_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(oil_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jFCDE2wL10QB"
      },
      "cell_type": "code",
      "source": [
        "print(\"=== Time Period: ===\")\n",
        "print(\"Begin:\", oil_df[\"date\"].min(), \"\\nEnd:\", oil_df[\"date\"].max())\n",
        "print(\"Tage insgesamt:\", oil_df[\"date\"].nunique())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "WzPY48k910QB"
      },
      "cell_type": "markdown",
      "source": [
        "#### holiday"
      ]
    },
    {
      "metadata": {
        "id": "9Iw2qGR410QB"
      },
      "cell_type": "code",
      "source": [
        "holidays_df = pd.read_csv(DATA_DIR / \"holidays_events.csv\",\n",
        "                          names=[\"date\", \"type\", \"local\", \"local-name\", \"description\", \"transferred\", ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(holidays_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(holidays_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(holidays_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "luHeP8uf10QB"
      },
      "cell_type": "code",
      "source": [
        "# 2. Missing Values\n",
        "print(\"=== How much NAs per Col: ===\")\n",
        "print(holidays_df.isnull().sum())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Cj2fqmth10QB"
      },
      "cell_type": "markdown",
      "source": [
        "#### Sample submission\n"
      ]
    },
    {
      "metadata": {
        "id": "5Ciat5ah10QC"
      },
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv(DATA_DIR / \"sample_submission.csv\",\n",
        "                          names=[\"id\", \"sales\" ],\n",
        "                          header=0)\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(sample_submission.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(sample_submission.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(sample_submission.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4yBtB-Sr10QC"
      },
      "cell_type": "markdown",
      "source": [
        "#### Stores"
      ]
    },
    {
      "metadata": {
        "id": "8dx6Fw3U10QC"
      },
      "cell_type": "code",
      "source": [
        "stores = pd.read_csv(DATA_DIR / \"stores.csv\",\n",
        "                          names=[\"store_nbr\", \"city\", \"state\", \"type\", \"cluster\" ],\n",
        "                          header=0)\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(stores.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(stores.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(stores.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "X8o6xYa_10QC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "store_counts = stores['type'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "store_counts.plot(kind='bar')\n",
        "plt.title(\"Distribution of Stores by Type\")\n",
        "plt.xlabel(\"Store Type\")\n",
        "plt.ylabel(\"Number of Stores\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fzS6BwI110QC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cluster_type_ct = pd.crosstab(stores['cluster'], stores['type'])\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cluster_type_ct, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
        "plt.title(\"Cluster vs. Store Type\")\n",
        "plt.ylabel(\"Cluster\")\n",
        "plt.xlabel(\"Store Type\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ElfRg2y510QC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "city_cluster_ct = pd.crosstab(stores['city'], stores['cluster'])\n",
        "\n",
        "values = range(int(city_cluster_ct.values.max()) + 1)\n",
        "cmap = ListedColormap(sns.color_palette(\"YlGnBu\", len(values)))\n",
        "norm = BoundaryNorm(values, cmap.N)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(\n",
        "    city_cluster_ct,\n",
        "    cmap=cmap,\n",
        "    norm=norm,\n",
        "    cbar=True,\n",
        "    linewidths=0.5,\n",
        "    linecolor=\"gray\",\n",
        "    annot=True, fmt=\"d\"\n",
        ")\n",
        "plt.title(\"City vs. Cluster Distribution (Discrete)\")\n",
        "plt.ylabel(\"City\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Qw3L7pDq10QC"
      },
      "cell_type": "markdown",
      "source": [
        "#### transaction\n"
      ]
    },
    {
      "metadata": {
        "id": "6w5gwjQ-10QD"
      },
      "cell_type": "code",
      "source": [
        "transaction_df = pd.read_csv(DATA_DIR / \"transactions.csv\",\n",
        "                          names=[\"date\", \"store_nbr\", \"transaction\" ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(transaction_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(transaction_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(transaction_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "69KxZaV910QD"
      },
      "cell_type": "code",
      "source": [
        "# count unique store_nbr\n",
        "transaction_df['store_nbr'].nunique()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bLVzryBM10QD"
      },
      "cell_type": "code",
      "source": [
        "transactions_per_day = transaction_df.groupby('date')['transaction'].sum()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "transactions_per_day.plot()\n",
        "plt.title(\"Total Transactions per Day\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Transactions\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "A2ElgQX910QD"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "transactions_per_day.plot(alpha=0.4, label=\"Daily\")\n",
        "transactions_per_day.rolling(7).mean().plot(label=\"7-day Avg\")\n",
        "transactions_per_day.rolling(14).mean().plot(label=\"14-day Avg\")\n",
        "transactions_per_day.rolling(30).mean().plot(label=\"30-day Avg\")\n",
        "plt.title(\"Total Transactions per Day with Rolling Averages\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Transactions\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cxiDoiay10QD"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train/ Test"
      ]
    },
    {
      "metadata": {
        "id": "ExoRNxQt10QD"
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(DATA_DIR / \"train.csv\",\n",
        "                          names=[\"id\", \"date\", \"store_nbr\", \"family\",\"sales\", \"onpromotion\" ],\n",
        "                          header=0, parse_dates=[\"date\"])\n",
        "\n",
        "print(\"=== Head: ===\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"=== Info: ===\")\n",
        "print(train_df.info())\n",
        "\n",
        "print(\"=== Description: ===\")\n",
        "print(train_df.describe())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VcNprnnX10QD"
      },
      "cell_type": "code",
      "source": [
        "train_df['family'].nunique()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Kbtx2pDg10QD"
      },
      "cell_type": "code",
      "source": [
        "# missing values\n",
        "train_df.isna().sum()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "IdVC0zJ110QD"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "daily_sales = train_df.groupby('date')['sales'].sum()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "ax = daily_sales.plot()\n",
        "\n",
        "# values into millions\n",
        "ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'{float(x/1e6)}M'))\n",
        "\n",
        "plt.title(\"Total Sales per Day\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "rJN-7jqP10QD"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "weekday_sales = train_df.groupby(train_df['date'].dt.day_name())['sales'].sum()\n",
        "\n",
        "# define order fpr weekdays\n",
        "order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "weekday_sales = weekday_sales.reindex(order)\n",
        "\n",
        "# values into millions\n",
        "ax = weekday_sales.plot.bar()\n",
        "ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'{float(x/1e6)}M'))\n",
        "ax.xaxis.set_tick_params(rotation=45)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "AjlyjCvD10QE"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.b Identifiy missing data and outliers"
      ]
    },
    {
      "metadata": {
        "id": "8MXsgi4T10QE"
      },
      "cell_type": "markdown",
      "source": [
        "#### missing data"
      ]
    },
    {
      "metadata": {
        "id": "76S1fif_10QE"
      },
      "cell_type": "code",
      "source": [
        "# Missing oil\n",
        "print(\"=== How much NAs per Col: ===\")\n",
        "print(oil_df.isnull().sum())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Fv0Wv2G510QE"
      },
      "cell_type": "code",
      "source": [
        "# Visualization to missing oil data:\n",
        "import matplotlib.pyplot as plt\n",
        "s = oil_df.set_index(\"date\")[\"oil_price\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,5))\n",
        "s.plot(ax=ax)\n",
        "\n",
        "for x in s.index[s.isna()]:\n",
        "    ax.axvline(x, linestyle=\"--\", linewidth=0.8, alpha=0.3, color=\"tab:orange\")\n",
        "\n",
        "ax.set_title(\"Daily Oil Price\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cuy4wTTu10QE"
      },
      "cell_type": "code",
      "source": [
        "tpd = (transaction_df\n",
        "       .groupby(['store_nbr', 'date'], as_index=False)['transaction']\n",
        "       .sum()\n",
        "       .sort_values(['store_nbr', 'date']))\n",
        "tpd['date'] = pd.to_datetime(tpd['date']).dt.normalize()\n",
        "\n",
        "\n",
        "full_idx = pd.date_range(tpd['date'].min(), tpd['date'].max(), freq='D')\n",
        "\n",
        "\n",
        "dates_by_store = tpd.groupby('store_nbr')['date'].unique()\n",
        "missing_counts = {s: len(full_idx.difference(pd.DatetimeIndex(dates)))\n",
        "                  for s, dates in dates_by_store.items()}\n",
        "stores_with_gaps = [s for s, cnt in missing_counts.items() if cnt > 0]\n",
        "\n",
        "stores_to_plot = sorted(stores_with_gaps, key=lambda s: missing_counts[s], reverse=True)\n",
        "\n",
        "# Subplots\n",
        "n = len(stores_to_plot)\n",
        "cols = 3 if n > 1 else 1\n",
        "rows = math.ceil(n / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14, 3.2*rows), sharex=True)\n",
        "axes = axes.flatten() if n > 1 else [axes]\n",
        "\n",
        "for ax, s in zip(axes, stores_to_plot):\n",
        "    sdf = (tpd[tpd['store_nbr'] == s]\n",
        "           .set_index('date')\n",
        "           .reindex(full_idx))\n",
        "    ax.plot(sdf.index, sdf['transaction'], linewidth=1.1)\n",
        "    # NA-days marked\n",
        "    miss = sdf['transaction'].isna()\n",
        "    if miss.any():\n",
        "        ymin, ymax = ax.get_ylim()\n",
        "        ax.scatter(sdf.index[miss], [ymin]*miss.sum(), marker='v', s=16, alpha=0.7, color=\"red\")\n",
        "    ax.set_title(f\"Store {s}  (missing days: {missing_counts[s]})\", fontsize=10)\n",
        "\n",
        "for ax in axes[n:]:\n",
        "    ax.axis('off')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "JhChGbaj10QE"
      },
      "cell_type": "markdown",
      "source": [
        "Most stores have 10-12 days missing around Christmas/ New Year, but some stores have a lot more missing data."
      ]
    },
    {
      "metadata": {
        "id": "5xguN_1210QE"
      },
      "cell_type": "code",
      "source": [
        "tpd_df = transaction_df.copy()\n",
        "tpd_df['date'] = pd.to_datetime(tpd_df['date']).dt.normalize()\n",
        "\n",
        "tpd = (tpd_df\n",
        "       .groupby(['store_nbr', 'date'], as_index=False)['transaction']\n",
        "       .sum()\n",
        "       .sort_values(['store_nbr', 'date']))\n",
        "\n",
        "\n",
        "stores_to_plot = sorted(tpd['store_nbr'].unique().tolist())\n",
        "\n",
        "\n",
        "n = len(stores_to_plot)\n",
        "cols = 4 if n >= 12 else 3 if n >= 6 else 2 if n > 1 else 1\n",
        "rows = math.ceil(n / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(3.2*cols, 3.8*rows))\n",
        "axes = axes.flatten() if n > 1 else [axes]\n",
        "\n",
        "for ax, store in zip(axes, stores_to_plot):\n",
        "    svals = tpd.loc[tpd['store_nbr'] == store, 'transaction']\n",
        "    sns.boxplot(y=svals, ax=ax, fliersize=2)\n",
        "    ax.set_title(f\"Store {store} (n={svals.notna().sum()})\", fontsize=9)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "\n",
        "for ax in axes[len(stores_to_plot):]:\n",
        "    ax.axis('off')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jdUb6PE810QE"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like there are outliers in every store transaction."
      ]
    },
    {
      "metadata": {
        "id": "jfi2XReU10QE"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Data Cleaning\n",
        "    a. Handling Missing Values\n",
        "    b. Choose appropriate methods to handle missing values (e.g., mean/median imputation for numerical data, mode imputation for categorical data, or deletion of rows/columns).\n",
        "    c. Justify your choices for handling missing data."
      ]
    },
    {
      "metadata": {
        "id": "Ctn6cqGM10QF"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HillBxOF10QF"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Handling Outliers\n",
        "    a. Detect outliers using methods such as the IQR method or Z-score.\n",
        "    b. Decide whether to remove, cap, or transform the outliers. Justify your decisions."
      ]
    },
    {
      "metadata": {
        "id": "M6j7ZN5h10QF"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YVxIqgca10QU"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 Data Transformation\n",
        "    a. Encoding Categorical Data\n",
        "        i. Apply label encoding or one-hot encoding to transform categorical data into numerical form.\n",
        "        ii. Justify your choice of encoding method.\n",
        "    b. Feature Scaling\n",
        "        i. Apply feature scaling techniques such as normalization (Min-Max scaling) or standardization (Z-score normalization) to the dataset.\n",
        "        ii. Explain why feature scaling is necessary and how it impacts the model.\n"
      ]
    },
    {
      "metadata": {
        "id": "PV74sH1E10QU"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eYdIHSGQ10QU"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 Data Splitting\n",
        "    a. Split the preprocessed dataset into training and testing sets. Typically, an 80-20 or 70-30 split is used.\n"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkICzAf110QU",
        "outputId": "85a651e7-6658-4cd3-d5d9-c2c2074ec908"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the training dataset\n",
        "train_df = pd.read_csv(\"train.csv\", parse_dates=[\"date\"])\n",
        "\n",
        "# Features and target\n",
        "X = train_df.drop(\"sales\", axis=1)\n",
        "y = train_df[\"sales\"]\n",
        "\n",
        "# Split 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Check the shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (383753, 5)\n",
            "X_test shape: (95939, 5)\n",
            "y_train shape: (383753,)\n",
            "y_test shape: (95939,)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "    b. Explain the importance of splitting the data and how it prevents overfitting."
      ],
      "metadata": {
        "id": "jRFJhzIK2FuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset into training (80%) and testing (20%) sets. The training set is used to teach the model, while the testing set evaluates its performance on unseen data. This prevents overfitting, ensuring the model generalizes well rather than just memorizing the training data."
      ],
      "metadata": {
        "id": "5bnA_nBI2cT0"
      }
    },
    {
      "metadata": {
        "id": "oKm_O51k10QU"
      },
      "cell_type": "markdown",
      "source": [
        "# 6 Bonus\n",
        "Apply dimensionality reduction techniques such as Principal\n",
        "Component Analysis (PCA) and discuss how it affects the dataset."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}